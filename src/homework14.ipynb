{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e147dbc",
   "metadata": {},
   "source": [
    "**Homework 14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae07e42e",
   "metadata": {},
   "source": [
    "*Background*\n",
    "\n",
    "Recall from the last assignment that Logistic Regression was a way to leverage linear regression to perform binary classification. Here we make further modifications to perform multi-class classification. This is  called *Softmax Regression*. \n",
    "\n",
    "As in Logistic Regression, the model we create will predict probabilities, and those probabilities will determine class predictions. The predictied probabilities will be a matrix of shape (n,k), where n is the number of observations and k is the number of classes. \n",
    "\n",
    "The input to the model is a feature matrix $X$ of shape (n,m), where as usual n is the number of observations, and m is the number of features. The model is then defined by a coefficient matrix of shape (m,k) and an intercept array of shape (k,). \n",
    "\n",
    "The first step in finding a matrix of predicted probabilites is to compute the matrix $$t=X \\cdot coef + intercept$$ (similar to linear regression, but note the different shapes!). The probability matrix $p$ is obtained from this matrix $t$ by the softmax function:\n",
    "$$p_i^j=\\frac{e^{t_i^j}}{\\sum \\limits _{j=1} ^k e^{t_i^j}}.$$\n",
    "\n",
    "To find the coefficient matrix and intercept array we must first preprocess the target array by doing a *one-hot encoding*. That is, we take a vector $y$ of $n$ entries, where each entry is one of $k$ different classes, and convert it to a matrix $Y$ of shape (n,k) whose entries are all 1's and 0's. In $Y$, a 1 in row i, column j indicates that $y_i$ is in category $j$. \n",
    "\n",
    "Our goal is to find the coefficent matrix and intercept array so that the probability $p_i^j$ is close to one if $Y_i^j=1$, and close to zero otherwise. In Softmax regression we accomplish this by using gradient descent to minimize the *categorical cross entropy loss*:\n",
    "$$CE=-\\frac{1}{n}\\sum \\limits _{i,j} Y_i^j \\mbox{Log}(p_i^j)$$\n",
    "\n",
    "With these definitions, the forumlas for the gradient calculation and coefficient/intercept updates are the same as in Logistic Regression. This should not be surprising, as binary Softmax Regression is mathematically identical to Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae2c2b",
   "metadata": {},
   "source": [
    "Before we define our `SoftmaxRegression()` class, we'll need to create a class that performs the one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,y):\n",
    "        self.categories= #Array of unique categories that appear in y\n",
    "        self.n_features_in=#Number of categories\n",
    "        \n",
    "    def transform(self,y):\n",
    "        Y=#Create one-hot encoding here (Hint: it's OK to use a for-loop to iterate through categories)\n",
    "        return Y\n",
    "    \n",
    "    def fit_transform(self,y):\n",
    "        '''Convenience method that applies fit and then \n",
    "        immediately transforms'''\n",
    "        #YOUR CODE HERE\n",
    "        return #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa2fc8",
   "metadata": {},
   "source": [
    "Test your class here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(['a','b','a','c','b','b'])\n",
    "encoder=OneHotEncoder()\n",
    "encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3881b4",
   "metadata": {},
   "source": [
    "Defining `OneHotEncoder()` as a class will allow us to fit our encoder to a train set but apply it to a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8596002",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.array(['b','c','b'])\n",
    "encoder.transform(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892d665",
   "metadata": {},
   "source": [
    "Note that this is a different result than if we had done `encoder.fit_transform(z)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c305a89",
   "metadata": {},
   "source": [
    "We are now ready to create our `SoftmaxRegression()` class. Below is most of the code from the `LogisticRegression()` class. Modify it appropriately, where indicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression():\n",
    "    def __init__(self,learning_rate, max_iter, batch_size, penalty='l2', alpha=0.0001):\n",
    "        self.lr=learning_rate\n",
    "        self.max_iter=max_iter \n",
    "        self.batch_size=batch_size\n",
    "        self.penalty=penalty \n",
    "        self.alpha=alpha \n",
    "        self.encoder=OneHotEncoder() \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        Y=self.encoder.fit_transform(y)\n",
    "        self.coef=np.ones(#What shape should be here?) \n",
    "        self.intercept=np.ones(#What shape should be here?) \n",
    "        if self.penalty=='l1':\n",
    "            penalty_grad=lambda x:2*(x>0)-1\n",
    "        elif self.penalty=='l2':\n",
    "            penalty_grad=lambda x:x\n",
    "        indices=np.arange(len(X))\n",
    "        for i in range(self.max_iter):\n",
    "            np.random.seed(i) \n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffle=X[indices] \n",
    "            Y_shuffle=Y[indices] \n",
    "            for j in range(0,len(X),self.batch_size):\n",
    "                X_batch=X_shuffle[j:j+self.batch_size]\n",
    "                Y_batch=Y_shuffle[j:j+self.batch_size] #Note that we're using Y here, not y\n",
    "                residuals=self.predict_proba(X_batch)-Y_batch \n",
    "                coef_grad=(X_batch.T)@residuals/len(X_batch)\n",
    "                intercept_grad=np.mean(residuals)\n",
    "                self.coef-=self.lr*coef_grad+self.alpha*penalty_grad(self.coef)\n",
    "                self.intercept-=self.lr*intercept_grad+self.alpha*penalty_grad(self.intercept)\n",
    "            \n",
    "    def predict_proba(self,X):\n",
    "        '''returns the matrix of predicted probabilites'''\n",
    "        #YOUR CODE HERE\n",
    "        return #YOUR CODE HERE\n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''returns a prediction, for each observation in X, \n",
    "        of one category.'''\n",
    "        #YOUR CODE HERE\n",
    "        return #YOUR CODE HERE\n",
    "    \n",
    "    def score(self,X,y): \n",
    "        '''returns accuracy of the model'''\n",
    "        return (self.predict(X)==y).mean() #Unchanged from Logistic Regression\n",
    "    \n",
    "    def CEloss(self,X,y): #Not a sklearn method!\n",
    "        '''returns the Categorical Cross Entropy loss'''\n",
    "        #YOUR CODE HERE\n",
    "        return #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee99da",
   "metadata": {},
   "source": [
    "Note that we have departed here from `sklearn` syntax. To perform Softmax Regression with that package, call the `LogisticRegression()` class and set `multi_class=multinomial`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14707ffe",
   "metadata": {},
   "source": [
    "We'll now test your `SoftmaxRegression()` class on some real data. To keep things realistic, we should first do a train/test split, so we'll need to bring back this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2434a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestSplit(x,y,p,seed=4):\n",
    "    '''Splits datasets x and y into train and test sets\n",
    "    p is the fraction going to train'''\n",
    "    rng = np.random.default_rng(seed=seed)#Don't change seed!\n",
    "    size=len(x)\n",
    "    train_size=int(p*size)\n",
    "    train_mask=np.zeros(size,dtype=bool)\n",
    "    train_indices=rng.choice(size, train_size, replace=False)  \n",
    "    train_mask[train_indices]=True\n",
    "    test_mask=~train_mask\n",
    "    x_train=x[train_mask]\n",
    "    x_test=x[test_mask]\n",
    "    y_train=y[train_mask]\n",
    "    y_test=y[test_mask]\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844396b",
   "metadata": {},
   "source": [
    "We'll now test your `SoftmaxRegression()` class on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaae8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=(pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv',index_col=0))\n",
    "X=np.array(iris.loc[:,'Sepal.Length':'Petal.Width'])\n",
    "y=np.array(iris['Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c908d5",
   "metadata": {},
   "source": [
    "Again, do an 80/20 train/test split. As this dataset is much smaller, your test set will only contain 30 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab18079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest=#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bffc3",
   "metadata": {},
   "source": [
    "Create a Softmax Regression object with a learning rate of 0.01, which trains for 1000 epochs with batches of size of 50, and no regularization. Then, fit it to `Xtrain` and `ytrain`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aece960",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=#YOUR CODE HERE\n",
    "mod.fit(#YOUR CODE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8443bb",
   "metadata": {},
   "source": [
    "Check the accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f75a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy=#YOUR CODE HERE\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea2b9ec",
   "metadata": {},
   "source": [
    "Multiplying this by 30 (the size of the test set) will reveal how many predictions were correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy*30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca70a0",
   "metadata": {},
   "source": [
    "List all 30 species that your model predicts from `Xtest`. (If you compare this to `ytest`, you would be able to determine which flower species the model had trouble with.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a294e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=#YOUR CODE HERE\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8ad61",
   "metadata": {},
   "source": [
    "Check the cross entropy loss on the test data. (This won't be very meaningful on its own, but would be useful in comparing the effects of different choices of `max_iter`, `batch_size`, learning rate, and amount of regularization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323cc8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss=#YOUR CODE HERE\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37947a61",
   "metadata": {},
   "source": [
    "What does your model say that the probability of a flower with Sepal Length 4, Sepal Width 3, Petal Length 2, and Petal Width 1 is of being a setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_prob=#YOUR CODE HERE\n",
    "setosa_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
