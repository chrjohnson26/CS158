{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsqtD65bt_XD"
   },
   "source": [
    "**Homework 26**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdM_EK8hhZBU"
   },
   "source": [
    "In this assignment you'll explore a sentiment analysis task. We start with the usual imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "pPqrGaF4NHsf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "device=('cuda' if torch.cuda.is_available()\n",
    "        else 'mps' if torch.backends.mps.is_available()\n",
    "        else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhxAHUuAxW4w"
   },
   "source": [
    "We now load the imdb movie review dataset that is packaged with keras. We'll only load the first 100 words of each review (if they are longer), and restrict to a 10,000 word vocabulary. The target variable y will be an array of 0's and 1's, indicating a positive or negative review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "rt8OJvhBxT0x"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(Xtrain,ytrain),(Xtest,ytest)=imdb.load_data(maxlen=100,num_words=10000,index_from=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVCMyEN9iBfJ"
   },
   "source": [
    "This dataset also comes with a dictionary for tokenization. The keys for this dictionary are the words in the vocabulary, and the values are the numbers assigned to each word (just like the `tokens` dictionary attribute of your `Tokenizer` class from the last assignment). We import this dictionary as `index`, and look at the first 10 key/value pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "wnPbw6xgNbEo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fawn', 34701),\n",
       " ('tsukino', 52006),\n",
       " ('nunnery', 52007),\n",
       " ('sonja', 16816),\n",
       " ('vani', 63951),\n",
       " ('woods', 1408),\n",
       " ('spiders', 16115),\n",
       " ('hanging', 2345),\n",
       " ('woody', 2289),\n",
       " ('trawling', 52008)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=imdb.get_word_index()\n",
    "list(index.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JveUPp1hlRSp"
   },
   "source": [
    "Note that Xtrain[0] is not the words of the first movie review of the dataset, but rather the indices of those words. To see the review itself, we have to convert from indices back to words. To this end, we'll build a dictionary word_from_index whose keys are indices and values are corresponding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "caQIPzAQlqxs"
   },
   "outputs": [],
   "source": [
    "words_from_index={}\n",
    "for word in index:\n",
    "  words_from_index[index[word]]=word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp3r_0nUnxDa"
   },
   "source": [
    "We can now look at review 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "zc2QoKSoOsqc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the when i rented this movie i had very low expectations but when i saw it i realized that the movie was less a lot less than what i expected the actors were bad the doctor's wife was one of the worst the story was so stupid it could work for a disney movie except for the murders but this one is not a comedy it is a laughable masterpiece of stupidity the title is well chosen except for one thing they could add stupid movie after dead husbands i give it 0 and a half out of 5 \""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review1=''\n",
    "for n in Xtrain[1]:\n",
    "  review1+=words_from_index[n]+' '\n",
    "# len(Xtrain[1])\n",
    "review1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob0wuGBKoXCT"
   },
   "source": [
    "Right now Xtrain is a 1-dimensional numpy array of python lists. To convert it to a 2-dimensional numpy array, we'll need every every element of Xtrain to be a list of the same length. Write a function that takes a list-of-lists, and adds an appropriate number of 0's to each if their length is less than 100. (This is called *padding*.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = len(Xtest)\n",
    "# temp = np.zeros(l*100)\n",
    "# # temp.c\n",
    "# num = temp.shape[0]\n",
    "# temp = temp.reshape(l, 100)\n",
    "# temp.shape\n",
    "temp = np.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "u6elT9Paqy8r"
   },
   "outputs": [],
   "source": [
    "def pad(X):\n",
    "  '''X is a list of lists of all different lengths\n",
    "  function should return a list of lists of length 100 by padding with 0's'''\n",
    "  x_len = X.shape[0]\n",
    "  new_lol = np.zeros(x_len*100, dtype=int)\n",
    "  new_lol = new_lol.reshape(x_len, 100)\n",
    "  \n",
    "  # Going through each list and adding to new_lol\n",
    "  for i in range(x_len):\n",
    "    l_len = len(X[i])\n",
    "    new_lol[i, :l_len]  = X[i]\n",
    "\n",
    "  return new_lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITTlcuH0vLM_"
   },
   "source": [
    "We'll now use this function to convert Xtrain and Xtest to 2D numpy arrays. (At the same time we convert ytrain and ytest to numpy arrays as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "6bf-zDt4RX-F"
   },
   "outputs": [],
   "source": [
    "Xtrain=np.array([np.array(l) for l in pad(Xtrain)], dtype=np.int64)\n",
    "Xtest=np.array([np.array(l) for l in pad(Xtest)], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V008DKjkzBgF"
   },
   "source": [
    "Finally, we convert all datasets to tensors and move them to the appropriate `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "AVC61B_3eym5"
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(Xtrain).to(device)\n",
    "y_train = torch.from_numpy(ytrain).to(device)\n",
    "X_test  = torch.from_numpy(Xtest).to(device)\n",
    "y_test  = torch.from_numpy(ytest).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVtEowCvvzUS"
   },
   "source": [
    "In this homework we introduce the Embedding layer, which has the same effect as a one-hot encoding followed by a dense layer. We also use a GRU layer in place of a RNN (feel free to try an RNN or an LSTM instead)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHGp06EmwVj_"
   },
   "source": [
    "Your model will consist of a Sequential Neural network with an Embedding layer and a GRU layer. For the embedding layer, the input_dim should be the size of your vocabulary, and the output_dim should be about 128 (you can play with that number). use 64 for the hidden dimension of the GRU, and don't forget to set `batch_first=True`.\n",
    "\n",
    "Follow this sequential model with a separate, fully-connected linear layer with an appropriate output size for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEfbJTHsfme3"
   },
   "outputs": [],
   "source": [
    "vocab_size = 29\n",
    "model=nn.Sequential(\n",
    "    nn.Embedding(vocab_size, 128), # Embedding layer\n",
    "    nn.GRU(128, 64, batch_first=True) # GRU layer\n",
    ").to(device)\n",
    "\n",
    "fc=nn.Linear(64, 2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3h9L__T0ihK"
   },
   "source": [
    "Define an optimizer that will simultaneously adjust the parameters of both `model` and `fc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "V3FrAVd8PAVe"
   },
   "outputs": [],
   "source": [
    "opt=torch.optim.Adam(list(model.parameters()) + list(fc.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc7VTyIv0tPK"
   },
   "source": [
    "Define a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "3Eolc5AthsxB"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-2k3ozp0vbq"
   },
   "source": [
    "Write your training loop! Use 100 epochs, with batches of size 32. Report losses as you train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "r17asLlZQzkl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, avg_loss: 0.33549837986295694\n",
      "epoch: 5, avg_loss: 0.31013016454678033\n",
      "epoch: 10, avg_loss: 0.28074329085596106\n",
      "epoch: 15, avg_loss: 0.24452823940846377\n",
      "epoch: 20, avg_loss: 0.19895589226958144\n",
      "epoch: 25, avg_loss: 0.1860415929697215\n",
      "epoch: 30, avg_loss: 0.12904205132727842\n",
      "epoch: 35, avg_loss: 0.08321246745862389\n",
      "epoch: 40, avg_loss: 0.08105885526334225\n",
      "epoch: 45, avg_loss: 0.05013198275612654\n",
      "epoch: 50, avg_loss: 0.0462831724513026\n",
      "epoch: 55, avg_loss: 0.05372289820695023\n",
      "epoch: 60, avg_loss: 0.025505408618697895\n",
      "epoch: 65, avg_loss: 0.04993217943997397\n",
      "epoch: 70, avg_loss: 0.08572232372009272\n",
      "epoch: 75, avg_loss: 0.012954247801082737\n",
      "epoch: 80, avg_loss: 0.0027624656838550646\n",
      "epoch: 85, avg_loss: 0.0005818421200191865\n",
      "epoch: 90, avg_loss: 0.0003054038652496846\n",
      "epoch: 95, avg_loss: 0.00019034788478815322\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "N = X_train.shape[0]\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    indices = torch.randperm(N, device=device)\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_y = y_train[batch_indices]\n",
    "\n",
    "        opt.zero_grad()\n",
    "        last_hidden = model(batch_X)[1].squeeze(0)\n",
    "        out = fc(last_hidden)\n",
    "        loss = criterion(out,batch_y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += loss.item()*batch_size\n",
    "\n",
    "    if epoch%5 ==0:\n",
    "        avg_loss = epoch_loss/(len(y_test) + len(y_train))\n",
    "        print(f\"epoch: {epoch}, avg_loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAW7GeEbwrQS"
   },
   "source": [
    "Evaluate the accuracy of your model on Xtest and ytest. (It should be around 80%.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 0, 1], device='mps:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "lh = model(X_test)[1].squeeze(0)\n",
    "out = fc(lh)\n",
    "out.shape\n",
    "out.sigmoid()\n",
    "preds = torch.argmax(out, dim=1)\n",
    "preds\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "VVdEkIshjxrq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3456, device='mps:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy= torch.sum(preds * y_test)/y_test.shape[0]\n",
    "\n",
    "accuracy\n",
    "\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3SmvNlz1AiE"
   },
   "source": [
    "Take a screen shot of this accuracy and upload to Gradescope!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPTKbRgTdBjKX2oppWIZepO",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
