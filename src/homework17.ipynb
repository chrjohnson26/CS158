{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRhTLUOx2st_"
   },
   "source": [
    "**Homework 17**\n",
    "\n",
    "In this assignment, you'll add to [Andrej Karpathy's \"micrograd\"](https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py), a simple immplementation of an autograd engine for scalar-valued functions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ppHuiGmveALx"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uit8usCxisKm"
   },
   "source": [
    "Add the indicated methods to the following class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rEVaV7pH1i5h"
   },
   "outputs": [],
   "source": [
    "class tensor():\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other): #self + other\n",
    "        out = tensor(self.data + other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other): #self * other\n",
    "        out = tensor(self.data * other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other): # self**n\n",
    "        out = tensor(self.data**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self): # self.exp()\n",
    "        out = tensor(np.exp(self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (np.exp(self.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def log(self): # self.log()\n",
    "        out = tensor(np.log(self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            # self.grad += (np.reciprocal(self.data)) * out.grad\n",
    "            self.grad += (self.data**-1) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def sin(self): # self.sin()\n",
    "        out = tensor(np.sin(self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (np.cos(self.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def cos(self): # self.cos()\n",
    "        out = tensor(np.cos(self.data),(self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (-np.sin(self.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self): # -self\n",
    "        return self * tensor(-1)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self, other): # self/other\n",
    "        return self * other**(-1)\n",
    "\n",
    "    def backward(self): #Implementation of backprop\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEgDGoBrjssZ"
   },
   "source": [
    "Let's make sure it works. First, we'll run this test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fKDmqOjhWTC8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tensor(2)\n",
    "b=tensor(3)\n",
    "d=a*(b**2)\n",
    "d.backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaHSYOT6kAZB"
   },
   "source": [
    "Check (by hand) that this is right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO5VMBGxkFxo"
   },
   "source": [
    "Next, observe what happens when we do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x6KcEjjRWYZc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 36)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lInNnd1kMqP"
   },
   "source": [
    "We now get the wrong answer because autograd engines often *accumulate* gradients, which causes problems when you differentiate more than once. To get the correct answer again, you have to manually zero out all gradients before recalculating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "skhB1v4wWaF3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#manually zero out gradients w.r.t. params:\n",
    "a.grad=0\n",
    "b.grad=0\n",
    "\n",
    "#rebuild computation graph:\n",
    "d=a*(b**2)\n",
    "\n",
    "#Calculate gradients:\n",
    "d.backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkEg-4RjlAKz"
   },
   "source": [
    "When we get to PyTorch in the next assignment you'll see this is a common workflow: Always zero out all gradients before calling the backward() method (especially when re-using tensors)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBSvNNEaliqW"
   },
   "source": [
    "Problem 1. Calculate the partial derivatives of the function $f(x,y)=\\frac{x^2+ e^y}{sin(xy)}$ where $x=-2$ and $y=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lXA78yL7WbWI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.709833962689768, -16.23170906228004)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tensor(-2)\n",
    "y = tensor(2)\n",
    "x.grad=0\n",
    "y.grad=0\n",
    "xy = x*y\n",
    "funct = ((x**2)+(y).exp())/((xy).sin())\n",
    "funct.backward()\n",
    "\n",
    "\n",
    "f_x=x.grad\n",
    "f_y=y.grad\n",
    "f_x, f_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btqYL8I6nI_N"
   },
   "source": [
    "Problem 2. Calculate the derviatve of $h(u)=\\log(\\sqrt u)$ where $u=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ei5j5Y14WlB2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tensor(4)\n",
    "u.grad = 0\n",
    "funct = (u**(1/2)).log()\n",
    "funct.backward()\n",
    "\n",
    "h_u=u.grad\n",
    "h_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyCpj1aAoB5l"
   },
   "source": [
    "Problem 3. Use gradient descent to find the values of $s$ and $t$ where there is a local minimum of $g(s,t)=\\frac{e^s}{s}+(\\log t)^2$. Start with $s=2$ and $t=3$. Use a learning rate of 0.001 and 10000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xm-vmwHZiYij"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0000000000031413, 1.0000001740908302)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=tensor(2)\n",
    "t=tensor(3)\n",
    "lr = 0.001\n",
    "for i in range(10000):\n",
    "    s.grad = 0\n",
    "    t.grad = 0\n",
    "    funct = (s.exp())/s + (t.log())**2\n",
    "    funct.backward()\n",
    "    s.data -= (lr)*s.grad\n",
    "    t.data -= (lr)*t.grad\n",
    "final_s=s.data\n",
    "final_t=t.data\n",
    "final_s,final_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1RykR9AWdIvmZXusY8IFVw_Z8to1L-Uf_",
     "timestamp": 1648433498372
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
