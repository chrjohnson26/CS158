{"cells":[{"cell_type":"markdown","metadata":{"id":"rxebqYTaL7Op"},"source":["**Homework 25**\n","\n","In this assignment your will train a RNN to predict characters of *Alice in Wonderland*, from strings of consecutive characters.\n","\n","We begin as usual with the imports you will need for this assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnuC9CayTbrH"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn"]},{"cell_type":"code","source":["device=('cuda' if torch.cuda.is_available()\n","        else 'cpu')\n","\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":56},"id":"Rdl0ITr5oWMz","executionInfo":{"status":"ok","timestamp":1745516974399,"user_tz":420,"elapsed":113,"user":{"displayName":"David Bachman","userId":"02952839939203243924"}},"outputId":"9b7125f3-a3ec-40a4-91a6-50aac5181982"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"6DcpEWAcsiWv"},"source":["Run the following text block to read *Alice in Wonderland* from the web, store it in the variable `text`, convert to lower case and remove punctuation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lETC1jDaQrKp"},"outputs":[],"source":["import string\n","from urllib.request import urlopen\n","url='https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt'\n","text = urlopen(url).read().decode('utf-8')\n","text=text.lower()\n","text=[c for c in text if (c not in string.punctuation) and (c!='\\n')]"]},{"cell_type":"markdown","metadata":{"id":"68T7cjSLRMEb"},"source":["Write a class `Tokenizer` with the following methods:\n","\n","\n","*   `__init__`, a method that builds a dictionary `tokens` whose keys are the set of unique characters in some input `text`, and values are integers.\n","*   `encode`, a method that takes in a corpus of text, converts each character according to the dictionary built by the __init__ method, and outputs a list of those integers.\n","*   `decode`, a method that takes a single integer (a value from the dictionary), and returns the corresponding character key.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBaSjQNWTEsB"},"outputs":[],"source":["class Tokenizer():\n","  def __init__(self,text):\n","    tokens={}\n","    n=0\n","    for c in text:\n","      if c not in tokens.keys():\n","        tokens[c]=n\n","        n+=1\n","    self.tokens=tokens\n","\n","  def encode(self,text):\n","    out=[]\n","    for c in text:\n","      out+=[self.tokens[c]]\n","    return out\n","\n","  def decode(self,n):\n","    for c in self.tokens:\n","      if n==self.tokens[c]:\n","        return c\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nTBoQ2PaU9I6"},"source":["Now, create an object called `tok` of your `Tokenizer` class, and use it to encode `text` as a list of integers, `text_indices`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJnStdvpVEyw"},"outputs":[],"source":["tok=Tokenizer(text)\n","text_indices=tok.encode(text)"]},{"cell_type":"markdown","metadata":{"id":"CMJOidgupSAY"},"source":["For convenience, we'll define `vocab_size=len(tok.tokens)` to be the length of your tokenizer dictionary:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9rHu8C_pYFf","executionInfo":{"status":"ok","timestamp":1745516974914,"user_tz":420,"elapsed":8,"user":{"displayName":"David Bachman","userId":"02952839939203243924"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0fdcae62-e91c-4d56-c486-26d44250ac2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["29"]},"metadata":{},"execution_count":6}],"source":["vocab_size=len(tok.tokens)\n","vocab_size"]},{"cell_type":"markdown","metadata":{"id":"r0F-VTPcsvCY"},"source":["The next task is to create feature sequences and targets. From `text_indices`, create a list-of-lists `X`. Each sublist of `X` should correspond to 50 consecutive elements of `text_indices`. At the same time, create a list `y` which contains the indices of the characters that follow each sublist of `X`. For example, `X[0]` should be a list containing the first 50 elements of `text_indices`: `text_indices[0]` through `text_indices[49]`. `y[0]` should be the 51st element, `text_indices[50]`.\n","\n","To keep the size of the feature and target vectors manageable, consecutive lists in `X` should be shifted by 3, so the overlap is 47 elements. Hence, `X[1]` should be a list containing the integers `text_indices[3]` through `text_indices[52]`, and `y[1]` should be the integer `text_indices[53]`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJ3XPYLUTjbA"},"outputs":[],"source":["seq_len=50\n","X=[]\n","y=[]\n","for i in range(0,len(text_indices)-seq_len-1,3):\n","  X.append(text_indices[i:i+seq_len])\n","  y.append(text_indices[i+seq_len])"]},{"cell_type":"markdown","metadata":{"id":"K5HcBpY_ut5T"},"source":["Convert `X` and `y` to numpy arrays with the same names, and check their shapes. If done correctly, the shape of `X` should be (45539, 50) and the shape of `y` should be (45539, ):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1202,"status":"ok","timestamp":1745516976287,"user":{"displayName":"David Bachman","userId":"02952839939203243924"},"user_tz":420},"id":"tc1lvY5wUE_o","outputId":"e396cccd-e191-40b6-d194-ac265bf27b84"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([45539, 50]), torch.Size([45539]))"]},"metadata":{},"execution_count":8}],"source":["X=torch.tensor(X).to(device)\n","y=torch.tensor(y).to(device)\n","X.shape, y.shape"]},{"cell_type":"markdown","metadata":{"id":"fYxo9IEjvLyR"},"source":["Use the `to_categorical` function again to convert both `X` and `y` to one-hot encoded vectors of 0's and 1's, and check their shapes again. You should now have shapes (45539,50,29) and (45539,29). In other words, the vector `X` now contains 45,539 sequences of length 50, and each element of each sequence is a 29-dimensional vector of 28 zeros and a single one in the entry corresponding to some character in the text."]},{"cell_type":"code","source":["import torch.nn.functional as F"],"metadata":{"id":"CVRpRZQP0Ezs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OneHotX=F.one_hot(X,vocab_size).float()"],"metadata":{"id":"GAkeXEhM0MOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OneHotX.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zq1vFCd00_jy","executionInfo":{"status":"ok","timestamp":1745516976386,"user_tz":420,"elapsed":6,"user":{"displayName":"David Bachman","userId":"02952839939203243924"}},"outputId":"9ad6ab22-568f-40f8-dfc4-fba73a2fce47"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([45539, 50, 29])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["'''\n","OneHotX=torch.zeros((X.shape[0],X.shape[1],vocab_size)).to(device)\n","for i in range(X.shape[0]):\n","  for j in range(X.shape[1]):\n","    OneHotX[i,j,X[i,j]]=1\n","'''"],"metadata":{"id":"2S10E7dJgrsj","executionInfo":{"status":"ok","timestamp":1745516976389,"user_tz":420,"elapsed":3,"user":{"displayName":"David Bachman","userId":"02952839939203243924"}},"colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"67a4f430-eb1c-452a-c1dc-73ae2c1f54c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nOneHotX=torch.zeros((X.shape[0],X.shape[1],vocab_size)).to(device)\\nfor i in range(X.shape[0]):\\n  for j in range(X.shape[1]):\\n    OneHotX[i,j,X[i,j]]=1\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Z9aYcAmwwncs"},"source":["You're now ready to create your model, which will consist of two seperate one-layer pytorch models. The first will be a recurrent layer that takes in sequences of 29-dimensional vectors, and has a 128 dimensional hidden state. The second will ve a linear layer that will take the last hidden state and produce a 29 dimensional vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0RbPYB0UmBp"},"outputs":[],"source":["rnn=nn.RNN(vocab_size,128,batch_first=True).to(device)\n","fc=nn.Linear(128,vocab_size).to(device)"]},{"cell_type":"markdown","metadata":{"id":"8iqIfJNyx1xL"},"source":["Compile your model using the `Adam` optimizer and an approporiately chosen loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfvSeb5cU8nj"},"outputs":[],"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(list(rnn.parameters()) + list(fc.parameters()), lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"-GIl_LiFx-zD"},"source":["Fit your data to X and y. Train for 50 epochs with a batch size of 128. Each epoch will take about 95 seconds, so you'll want to leave your computer for about an hour for this to complete."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":184108,"status":"ok","timestamp":1745517166955,"user":{"displayName":"David Bachman","userId":"02952839939203243924"},"user_tz":420},"id":"9HMXgjxKVJb2","outputId":"9770521b-04ca-4281-dfb9-b6e68b4bce43"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, avg_loss: 2.362585588155856\n","epoch: 2, avg_loss: 1.9816782456769542\n","epoch: 4, avg_loss: 1.8501564488192839\n","epoch: 6, avg_loss: 1.7504945752198664\n","epoch: 8, avg_loss: 1.678781470363666\n","epoch: 10, avg_loss: 1.615750982548875\n","epoch: 12, avg_loss: 1.566183029687798\n","epoch: 14, avg_loss: 1.5211048758666834\n","epoch: 16, avg_loss: 1.4840404801004428\n","epoch: 18, avg_loss: 1.4528725123844712\n","epoch: 20, avg_loss: 1.4242201379805195\n","epoch: 22, avg_loss: 1.3963262983774616\n","epoch: 24, avg_loss: 1.3751589620680638\n","epoch: 26, avg_loss: 1.3546202259888414\n","epoch: 28, avg_loss: 1.3412804920742112\n","epoch: 30, avg_loss: 1.325974803026487\n","epoch: 32, avg_loss: 1.3119116378774462\n","epoch: 34, avg_loss: 1.2990341318457563\n","epoch: 36, avg_loss: 1.2888927964417185\n","epoch: 38, avg_loss: 1.2765215783830284\n","epoch: 40, avg_loss: 1.2700999458226876\n","epoch: 42, avg_loss: 1.2654386535798998\n","epoch: 44, avg_loss: 1.255694712916943\n","epoch: 46, avg_loss: 1.2511304133771717\n","epoch: 48, avg_loss: 1.2472890148432154\n","epoch: 50, avg_loss: 1.2449930631825674\n","epoch: 52, avg_loss: 1.2387893528237681\n","epoch: 54, avg_loss: 1.2332142314493306\n","epoch: 56, avg_loss: 1.232419964049356\n","epoch: 58, avg_loss: 1.2251065536780819\n","epoch: 60, avg_loss: 1.2286975248211935\n","epoch: 62, avg_loss: 1.2227790375943686\n","epoch: 64, avg_loss: 1.2224962041454408\n","epoch: 66, avg_loss: 1.2182531997843844\n","epoch: 68, avg_loss: 1.2170450751438382\n","epoch: 70, avg_loss: 1.2135415386835833\n","epoch: 72, avg_loss: 1.2102556168115532\n","epoch: 74, avg_loss: 1.210569976799604\n"]}],"source":["n_epochs=75\n","N = OneHotX.shape[0]  # total number of observations in training data\n","batch_size=32\n","\n","rnn.train()\n","for epoch in range(n_epochs):\n","  epoch_loss = 0.0\n","\n","  # Shuffle the indices\n","  indices = torch.randperm(N,device=device)\n","\n","  # Create mini-batches\n","  for i in range(0, N, batch_size):\n","    batch_indices = indices[i:i+batch_size]\n","    batch_X = OneHotX[batch_indices]\n","    batch_y = y[batch_indices]\n","\n","    optimizer.zero_grad()\n","    last_hidden=lstm(batch_X)[1][0].squeeze(0)\n","    out=fc(last_hidden)\n","    loss=criterion(out,batch_y)\n","    loss.backward()\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()*batch_size\n","\n","  if epoch%2==0:\n","      avg_loss = epoch_loss / len(y)\n","      print(f\"epoch: {epoch}, avg_loss: {avg_loss}\")"]},{"cell_type":"markdown","metadata":{"id":"QQcsGH5Jyb7e"},"source":["We will now use your trained model to generate text, one character at a time. Run the following code block to do this. (It will take a minute or two to complete.) Its interesting that although the model generates one character at a time, you'll see very word-like strings in the final text."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":303,"status":"ok","timestamp":1745517205421,"user":{"displayName":"David Bachman","userId":"02952839939203243924"},"user_tz":420},"id":"t4IrfYOGaVHn","colab":{"base_uri":"https://localhost:8080/","height":160},"outputId":"c0855a2f-3b0c-4769-e8af-32591349a397"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ce and taremint  its greeth this with a cut lersterk and wan ive too bolk an orfond the bohind  a worde him  the hatter when with ened quite and the dayture amout that be puzzer ther she who very suppoce horde then i perpan said the duchess mad feer arimbelly mouse  ho hees andshe doumor to be a fildipes and mofy drep on fent so she spoke tive      hememboor alice she found at there and of hard the mock turtlist groind iow  i dorfive you said the mus a sing  pokies heast idaly fan of the queen p'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["rnn.eval()\n","next_seq=OneHotX[:1]\n","\n","newtext=''\n","with torch.no_grad():\n","  for i in range(500):\n","    seq=next_seq\n","    pred=fc(rnn(seq)[1].squeeze()) #predictions of your model\n","    pred_probs=torch.softmax(pred,dim=0).detach().cpu().numpy() #predictions->probs\n","    index_pred=np.random.choice(vocab_size,1,p=pred_probs)[0] #choose one\n","    newtext+=tok.decode(index_pred) #corresponding character\n","\n","    next_vec=torch.zeros(vocab_size).to(device)\n","    next_vec[index_pred]=1  #one-hot encode chosen letter index\n","    next_seq=torch.zeros(1,seq_len,29).to(device)\n","    next_seq[0,:seq_len-1]=seq[0,1:] #new sequence is last 49 of old sequence\n","    next_seq[0,seq_len-1]=next_vec  #plus new vector\n","\n","newtext #display generated text"]},{"cell_type":"markdown","metadata":{"id":"nk9dRoo_0Ok_"},"source":["**COPY AND PASTE THIS TEXT INTO THE SUBMISSION WINDOW ON GRADESCOPE**"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1a7fdaeftlTzb6zEdH-idQ13R4Jm2_h8K","timestamp":1650312231146}],"authorship_tag":"ABX9TyPh5/Ar3r9TNua7HlI8k0QJ"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}