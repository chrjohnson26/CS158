{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2-_ZEnR0sjK"
   },
   "source": [
    "homework 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0428G5XVS0b"
   },
   "source": [
    "We begin by importing the regression versions of the models you learned about in previous assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fJUdREEf0gBc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVKOx3VJXIwi"
   },
   "source": [
    "In this assignment you'll code up a Gradient Boosting Classifier. Such a model trains a sequence of decision trees, where each one predicts the gradient of the (negative) log loss of $\\sigma(t)$, where $t$ is the sum of all previous trees (with a learning rate applied to each for stability). Setting `n_estimators=1` would  produce a single decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDOkE2L8TRG8"
   },
   "source": [
    "For reference, here is code for a Gradient Boosting Regressor. You'll modify this to make the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9G9uo3zJTgHd"
   },
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor():\n",
    "  def __init__(self,learning_rate, n_estimators, max_depth):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.n_estimators = n_estimators\n",
    "    self.max_depth = max_depth\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    self.estimators = []\n",
    "    tree = DecisionTreeRegressor(max_depth=self.max_depth,random_state=42)\n",
    "    tree.fit(X, y)\n",
    "    self.estimators.append(tree)\n",
    "    current_prediction = tree.predict(X)\n",
    "\n",
    "    for _ in range(1,self.n_estimators):\n",
    "      residuals = y - current_prediction\n",
    "      tree = DecisionTreeRegressor(max_depth=self.max_depth,random_state=42)\n",
    "      tree.fit(X, residuals)\n",
    "      self.estimators.append(tree)\n",
    "      current_prediction += self.learning_rate * tree.predict(X)\n",
    "\n",
    "  def predict(self, X):\n",
    "    predictions=self.estimators[0].predict(X)\n",
    "    for i in range(1,self.n_estimators):\n",
    "      predictions += self.learning_rate * self.estimators[i].predict(X)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfjVifr0S1Ak"
   },
   "source": [
    "To make this into a classifier, you'll need to define two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IucWqBXkS8pr"
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "  '''returns the sigmoid of t'''\n",
    "  return 1. / (1. + np.exp(-t))\n",
    "\n",
    "def log_odds(p):\n",
    "  '''returns the log odds of p'''\n",
    "  return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uou4c7JxTNNE"
   },
   "source": [
    "Now, modify the above regressor to make a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lLh-RSs7OiJT"
   },
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier():\n",
    "  def __init__(self,learning_rate, n_estimators, max_depth):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.n_estimators = n_estimators\n",
    "    self.max_depth = max_depth\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    self.estimators = []\n",
    "    self.initial_prediction = log_odds(np.mean(y))\n",
    "    current_prediction = self.initial_prediction\n",
    "    for _ in range(0,self.n_estimators):\n",
    "      residuals = y - sigmoid(current_prediction)\n",
    "      tree = DecisionTreeRegressor(max_depth=self.max_depth,random_state=1)\n",
    "      tree.fit(X, residuals)\n",
    "      self.estimators.append(tree)\n",
    "      current_prediction += self.learning_rate * tree.predict(X)\n",
    "\n",
    "  def predict_log_proba(self, X):\n",
    "    '''returns the log of the probability of class 1, i.e., the\n",
    "    output of the weighted sum of all trees'''\n",
    "    predictions = self.initial_prediction*np.ones(len(X))\n",
    "    for i in range(0,self.n_estimators):\n",
    "      predictions += self.learning_rate * self.estimators[i].predict(X)\n",
    "    return predictions\n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    '''returns the probability of class 1, which is\n",
    "    the sigmoid of the log probabilities'''\n",
    "    return sigmoid(self.predict_log_proba(X))\n",
    "\n",
    "  def predict(self, X):\n",
    "    '''returns the class predictions:\n",
    "    0 if prob<0.5, and 1 if prob>=0.5 '''\n",
    "    preds = self.predict_proba(X)\n",
    "    return np.round(preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0WxFIdDT82r"
   },
   "source": [
    "To test the accuracy of your model, we'll create a challenging synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KhbvblPqQPqQ"
   },
   "outputs": [],
   "source": [
    "#Code courtesy of ChatGPT\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,    # number of samples\n",
    "    n_features=20,      # total number of features\n",
    "    n_informative=10,   # number of features actually informative\n",
    "    n_redundant=5,      # number of redundant features\n",
    "    n_classes=2,        # binary classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgyGT8r9ULuR"
   },
   "source": [
    "Check the accuracy *on the test data* of a Gradient Boosting Classifier model with learning rate=0.1, 20 estimators, and a max_depth of 5 which has been trained *on the train data.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CBApAbjIMqFi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8556666666666667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = GradientBoostingClassifier(learning_rate=.1, n_estimators=20, max_depth=5)\n",
    "mod.fit(train_data, train_target)\n",
    "preds = mod.predict(test_data)\n",
    "accuracy= np.sum((preds == test_target))/test_target.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO3Hrd/rSmUtOy/e1NyXEHQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
